{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c22ca771",
   "metadata": {},
   "source": [
    "# Titanic Project\n",
    "Problem Statement:\n",
    " The Titanic Problem is based on the sinking of the ‘Unsinkable’ ship Titanic in early 1912. It gives you information about multiple people like their ages, sexes, sibling counts, embarkment points, and whether or not they survived the disaster. Based on these features, you have to predict if an arbitrary passenger on Titanic would survive the sinking or not. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae162ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing pandas library\n",
    "import pandas as pd\n",
    " \n",
    "#loading data\n",
    "titanic = pd.read_csv(\"C:\\\\Users\\\\win 7\\\\Desktop\\\\Datascience\\\\Titanic Project.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9936f8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426e010d",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60e16b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# Countplot\n",
    "sns.catplot(x =\"Sex\", hue =\"Survived\",\n",
    "kind =\"count\", data = titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1e0871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the dataset by Pclass and Survived and then unstack them\n",
    "group = titanic.groupby(['Pclass', 'Survived'])\n",
    "pclass_survived = group.size().unstack()\n",
    " \n",
    "# Heatmap - Color encoded 2D representation of data.\n",
    "sns.heatmap(pclass_survived, annot = True, fmt =\"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee756f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Violinplot Displays distribution of data\n",
    "# across all levels of a category.\n",
    "sns.violinplot(x =\"Sex\", y =\"Age\", hue =\"Survived\",\n",
    "data = titanic, split = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e5e399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a column Family_Size\n",
    "titanic['Family_Size'] = 0\n",
    "titanic['Family_Size'] = titanic['Parch']+titanic['SibSp']\n",
    " \n",
    "# Adding a column Alone\n",
    "titanic['Alone'] = 0\n",
    "titanic.loc[titanic.Family_Size == 0, 'Alone'] = 1\n",
    " \n",
    "# Factorplot for Family_Size\n",
    "sns.factorplot(x ='Family_Size', y ='Survived', data = titanic)\n",
    " \n",
    "# Factorplot for Alone\n",
    "sns.factorplot(x ='Alone', y ='Survived', data = titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd98b80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide Fare into 4 bins\n",
    "titanic['Fare_Range'] = pd.qcut(titanic['Fare'], 4)\n",
    " \n",
    "# Barplot - Shows approximate values based\n",
    "# on the height of bars.\n",
    "sns.barplot(x ='Fare_Range', y ='Survived',\n",
    "data = titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5b432e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countplot\n",
    "sns.catplot(x ='Embarked', hue ='Survived',\n",
    "kind ='count', col ='Pclass', data = titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ef76d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184bfeed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "241b3a3f",
   "metadata": {},
   "source": [
    "# Advertising Sales Channel Prediction\n",
    "Problem Statement:\n",
    "Sales Channel Prediction Case Study  \n",
    "\n",
    "When a company enters a market, the distribution strategy and channel it uses are keys to its success in the market, as well as market know-how and customer knowledge and understanding. Because an effective distribution strategy under efficient supply-chain management opens doors for attaining competitive advantage and strong brand equity in the market, it is a component of the marketing mix that cannot be ignored . \n",
    "\n",
    "The distribution strategy and the channel design have to be right the first time. The case study of Sales channel includes the detailed study of TV, radio and newspaper channel. The predict the total sales generated from all the sales channel.  \n",
    "\n",
    "The below link provided is for downloading the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c71cc21",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\WIN7~1\\AppData\\Local\\Temp/ipykernel_12652/1767305750.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m                      \u001b[1;31m#used to convert input into numpy arrays to be fed to the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m         \u001b[1;31m#to plot/visualize sales data and sales forecasting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m                 \u001b[1;31m# acts as the framework upon which this model is built\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m            \u001b[1;31m#defines layers and functions in the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import pandas as pd                     # to extract data from dataset(.csv file)\n",
    "import csv                              #used to read and write to csv files\n",
    "import numpy as np                      #used to convert input into numpy arrays to be fed to the model\n",
    "import matplotlib.pyplot as plt         #to plot/visualize sales data and sales forecasting\n",
    "import tensorflow as tf                 # acts as the framework upon which this model is built\n",
    "from tensorflow import keras            #defines layers and functions in the model\n",
    " \n",
    "#here the csv file has been copied into three lists to allow better availability\n",
    "list_row,date,traffic = get_data(r'C:\\Users\\win 7\\Desktop\\Datascience\\Advertising.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbeaf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversion(week,days,months,years,list_row):\n",
    "  #lists have been defined to hold different inputs\n",
    "  inp_day = []\n",
    "  inp_mon = []\n",
    "  inp_year = []\n",
    "  inp_week=[]\n",
    "  inp_hol=[]\n",
    "  out = []\n",
    "  #converts the days of a week(monday,sunday,etc.) into one hot vectors and stores them as a dictionary\n",
    "  week1 = number_to_one_hot(week)\n",
    "  #list_row contains primary inputs\n",
    "  for row in list_row:\n",
    "        #Filter out date from list_row\n",
    "        d = row[0]\n",
    "        #the date was split into three values date, month and year.\n",
    "        d_split=d.split('/')\n",
    "        if d_split[2]==str(year_all[0]):\n",
    "          #prevents use of the first year data to ensure each input contains previous year data as well.\n",
    "          continue\n",
    "        #encode the three parameters of date into one hot vectors using date_to_enc function.\n",
    "        d1,m1,y1 = date_to_enc(d,days,months,years) #days, months and years and dictionaries containing the one hot encoding of each date,month and year.\n",
    "        inp_day.append(d1) #append date into date input\n",
    "        inp_mon.append(m1) #append month into month input\n",
    "         inp_year.append(y1) #append year into year input\n",
    "        week2 = week1[row[3]] #the day column from list_is converted into its one-hot representation and saved into week2 variable\n",
    "        inp_week.append(week2)# it is now appended into week input.\n",
    "        inp_hol.append([row[2]])#specifies whether the day is a holiday or not\n",
    "        t1 = row[1] #row[1] contains the traffic/sales value for a specific date\n",
    "        out.append(t1) #append t1(traffic value) into a list out\n",
    "  return inp_day,inp_mon,inp_year,inp_week,inp_hol,out #all the processed inputs are returned\n",
    " \n",
    "inp_day,inp_mon,inp_year,inp_week,inp_hol,out = conversion(week,days,months,years,list_train)\n",
    "#all of the inputs must be converted into numpy arrays to be fed into the model\n",
    "inp_day = np.array(inp_day)\n",
    "inp_mon = np.array(inp_mon)\n",
    "inp_year = np.array(inp_year)\n",
    "inp_week = np.array(inp_week)\n",
    "inp_hol = np.array(inp_hol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d361cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def other_inputs(season,list_row):\n",
    "  #lists to hold all the inputs\n",
    "  inp7=[]\n",
    "  inp_prev=[]\n",
    "  inp_sess=[]\n",
    "  count=0 #count variable will be used to keep track of the index of current row in order to access the traffic values of past seven days.\n",
    "  for row in list_row:\n",
    "    ind = count\n",
    "    count=count+1\n",
    "    d = row[0] #date was copied to variable d\n",
    "    d_split=d.split('/')\n",
    "    if d_split[2]==str(year_all[0]):\n",
    "      #preventing use of the first year in the data\n",
    "      continue\n",
    "    sess = cur_season(season,d) #assigning a season to to the current date\n",
    "    inp_sess.append(sess) #appending sess variable to an input list\n",
    "    t7=[] #temporary list to hold seven sales value\n",
    "    t_prev=[] #temporary list to hold the previous year sales value\n",
    "    t_prev.append(list_row[ind-365][1]) #accessing the sales value from one year back and appending them\n",
    "    for j in range(0,7):\n",
    "        t7.append(list_row[ind-j-1][1]) #appending the last seven days sales value\n",
    "    inp7.append(t7)\n",
    "    inp_prev.append(t_prev)\n",
    "  return inp7,inp_prev,inp_sess\n",
    "    inp7,inp_prev,inp_sess = other_inputs(season,list_train)\n",
    "inp7 = np.array(inp7)\n",
    "inp7= inp7.reshape(inp7.shape[0],inp7.shape[1],1)\n",
    "inp_prev = np.array(inp_prev)\n",
    "inp_sess = np.array(inp_sess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443678c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense,LSTM,Flatten\n",
    "from tensorflow.keras.layers import concatenate\n",
    "#an Input variable is made from every input array\n",
    "input_day = Input(shape=(inp_day.shape[1],),name = 'input_day')\n",
    "input_mon = Input(shape=(inp_mon.shape[1],),name = 'input_mon')\n",
    "input_year = Input(shape=(inp_year.shape[1],),name = 'input_year')\n",
    "input_week = Input(shape=(inp_week.shape[1],),name = 'input_week')\n",
    "input_hol = Input(shape=(inp_hol.shape[1],),name = 'input_hol')\n",
    "input_day7 = Input(shape=(inp7.shape[1],inp7.shape[2]),name = 'input_day7')\n",
    "input_day_prev = Input(shape=(inp_prev.shape[1],),name = 'input_day_prev')\n",
    "input_day_sess = Input(shape=(inp_sess.shape[1],),name = 'input_day_sess')\n",
    "# The model is quite straight-forward, all inputs were inserted into a dense layer with 5 units and 'relu' as activation function\n",
    "x1 = Dense(5, activation='relu')(input_day)\n",
    "x2 = Dense(5, activation='relu')(input_mon)\n",
    "x3 = Dense(5, activation='relu')(input_year)\n",
    "x4 = Dense(5, activation='relu')(input_week)\n",
    "x5 = Dense(5, activation='relu')(input_hol)\n",
    "x_6 = Dense(5, activation='relu')(input_day7)\n",
    "x__6 = LSTM(5,return_sequences=True)(x_6) # LSTM is used to remember the importance of each day from the seven days data\n",
    "x6 = Flatten()(x__10) # done to make the shape compatible to other inputs as LSTM outputs a three dimensional tensor\n",
    "x7 = Dense(5, activation='relu')(input_day_prev)\n",
    "x8 = Dense(5, activation='relu')(input_day_sess)\n",
    "c = concatenate([x1,x2,x3,x4,x5,x6,x7,x8]) # all inputs are concatenated into one\n",
    "layer1 = Dense(64,activation='relu')(c)\n",
    "outputs = Dense(1, activation='sigmoid')(layer1) # a single output is produced with value ranging between 0-1.\n",
    "# now the model is initialized and created as well\n",
    "model = Model(inputs=[input_day,input_mon,input_year,input_week,input_hol,input_day7,input_day_prev,input_day_sess], outputs=outputs)\n",
    "model.summary() # used to draw a summary(diagram) of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f40bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    " \n",
    "model.compile(loss=['mean_squared_error'],\n",
    "              optimizer = 'adam',\n",
    "              metrics = ['acc'] #while accuracy is used as a metrics here it will remain zero as this is no classification model\n",
    "              )                  # linear regression models are best gauged by their loss value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f86a56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "           x = [inp_day,inp_mon,inp_year,inp_week,inp_hol,inp7,inp_prev,inp_sess],\n",
    "           y = out,\n",
    "           batch_size=16,\n",
    "           steps_per_epoch=50,\n",
    "           epochs = 15,\n",
    "           verbose=1,\n",
    "           shuffle =False\n",
    "           )\n",
    "#all the inputs were fed into the model and the training was completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0183a741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input(date):\n",
    "    d1,d2,d3 = date_to_enc(date,days,months,years)   #separate date into three parameters\n",
    "    print('date=',date)\n",
    "    d1 = np.array([d1])                        \n",
    "    d2 = np.array([d2])\n",
    "    d3 = np.array([d3])\n",
    "    week1 = number_to_one_hot(week)        #defining one hot vector to encode days of a week\n",
    "    week2 = week1[day[date]]\n",
    "    week2=np.array([week2])\n",
    "    //appeding a column for holiday(0-not holiday, 1- holiday)\n",
    "    if date in holiday:\n",
    "        h=1\n",
    "        #print('holiday')\n",
    "    else:\n",
    "        h=0\n",
    "        #print(\"no holiday\")\n",
    "    h = np.array([h])\n",
    "    sess = cur_season(season,date)        #getting seasonality data from cur_season function\n",
    "    sess = np.array([sess])                            \n",
    "    return d1,d2,d3,week2,h,sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a27fb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_testing(date):\n",
    "maxj = max(traffic) # determines the maximum sales value in order to normalize or return the data to its original form\n",
    "out=[]\n",
    "count=-1\n",
    "ind=0\n",
    "for i in list_row:\n",
    "count =count+1\n",
    "if i[0]==date: #identify the index of the data in list\n",
    "ind = count\n",
    "t7=[]\n",
    "t_prev=[]\n",
    "t_prev.append(list_row[ind-365][1]) #previous year data\n",
    "# for the first input, sales data of last seven days will be taken from training data\n",
    "for j in range(0,7):\n",
    "t7.append(list_row[ind-j-365][1])\n",
    "result=[] # list to store the output and values\n",
    "count=0\n",
    "for i in list_date[ind-364:ind+2]:\n",
    "d1,d2,d3,week2,h,sess = input(i) # using input function to process input values into numpy arrays\n",
    "t_7 = np.array([t7]) # converting the data into a numpy array\n",
    "t_7 = t_7.reshape(1,7,1)\n",
    "# extracting and processing the previous year sales value\n",
    "t_prev=[]\n",
    "t_prev.append(list_row[ind-730+count][1])\n",
    "t_prev = np.array([t_prev])\n",
    "#predicting value for output\n",
    "y_out = model.predict([d1,d2,d3,week2,h,t_7,t_prev,sess])\n",
    "#output and multiply the max value to the output value to increase its range from 0-1\n",
    "print(y_out[0][0]*maxj)\n",
    "t7.pop(0) #delete the first value from the last seven days value\n",
    "t7.append(y_out[0][0]) # append the output as input for the seven days data\n",
    "result.append(y_out[0][0]*maxj) # append the output value to the result list\n",
    "count=count+1\n",
    "return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82678af",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(result,color='red',label='predicted')\n",
    "plt.plot(test_sales,color='purple',label=\"actual\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sales\")\n",
    "leg = plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288971aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0038f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de376026",
   "metadata": {},
   "source": [
    "# Big Data Mart Sales Problem\n",
    "Problem Statement:\n",
    "The data scientists at BigMart have collected 2013 sales data for 1559 products across 10 stores in different cities. Also, certain attributes of each product and store have been defined. The aim is to build a predictive model and find out the sales of each product at a particular store.\n",
    "\n",
    "Using this model, BigMart will try to understand the properties of products and stores which play a key role in increasing the sales of their products.\n",
    "\n",
    "The dataset includes two files:\n",
    "\n",
    "- bigdatamart_Train.csv: Use this file for the model building purpose. \n",
    "\n",
    "- bigdatamart_Test.csv: Use this file for getting predictions from the trained model. \n",
    "\n",
    "Note: You can find the dataset in the link below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9ace644",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas_profiling'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\WIN7~1\\AppData\\Local\\Temp/ipykernel_12652/3324604552.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m \u001b[1;31m# data processing, CSV file I/O (e.g. pd.read_csv)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas_profiling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m# Input data files are available in the \"../input/\" directory.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas_profiling'"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas_profiling\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('https://github.com/dsrscientist/bigdatamart_rep'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66bccc3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 28, saw 493\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\WIN7~1\\AppData\\Local\\Temp/ipykernel_12652/2328101906.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"https://github.com/dsrscientist/bigdatamart_rep/blob/master/bigdatamart_Test.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 488\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1047\u001b[1;33m         \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1049\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m                 \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m                 \u001b[1;31m# destructive to chunks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 28, saw 493\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"https://github.com/dsrscientist/bigdatamart_rep/blob/master/bigdatamart_Test.csv\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b143d2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.profile_report(style={'full_width':True},title='Training dataset Profiling Report')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a132cd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns in training dataset based on datatypes {}\".format(df_train.columns.to_series().groupby(df_train.dtypes).groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe849dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e544b240",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad9cb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Item_Weight']=df_train['Item_Weight'].fillna(df_train['Item_Weight'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8315448b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Outlet_Size']=df_train['Outlet_Size'].fillna(df_train['Outlet_Size'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ae6dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(df_train.isnull(),yticklabels=False,cbar=False,cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1957be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeef1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8b5933",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Outlet_Identifier'].value_counts().plot(kind='bar',color = 'Black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70a06fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(['Item_Identifier','Outlet_Identifier'],axis=1)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c336b243",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Item_Fat_Content'].value_counts().plot(kind='bar',color = 'black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8e756b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train =  df_train.replace(to_replace =\"low fat\",  value =\"Low Fat\") \n",
    "df_train =  df_train.replace(to_replace =\"LF\",  value =\"Low Fat\") \n",
    "df_train =  df_train.replace(to_replace =\"reg\",  value =\"Regular\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b2dc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Item_Fat_Content'].value_counts().plot(kind='bar',color = 'Green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760fcbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Item_Type'].value_counts().plot(kind='bar',color = 'Green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68a3421",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Outlet_Size'].value_counts().plot(kind='bar',color = 'green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ab4961",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Outlet_Location_Type'].value_counts().plot(kind='bar',color = 'Green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446613c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Outlet_Type'].value_counts().plot(kind='bar',color = 'green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98b78ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_train['Item_Weight']\n",
    "plt.figure(1); \n",
    "sns.distplot(y, kde=True,color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fa6b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_train['Item_Visibility']\n",
    "plt.figure(1); \n",
    "sns.distplot(y, kde=True,color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927f5dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_train['Item_MRP']\n",
    "plt.figure(1);\n",
    "sns.distplot(y, kde=True,color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd5b91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_train['Outlet_Establishment_Year']\n",
    "plt.figure(1); \n",
    "sns.distplot(y, kde=True,color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5bd3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_train['Item_Outlet_Sales']\n",
    "plt.figure(1);\n",
    "sns.distplot(y, kde=True,color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecace81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Qty_Sold\"] = (df_train[\"Item_Outlet_Sales\"]/df_train[\"Item_MRP\"])\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2553c1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(df_train.corr(),annot=True,cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdde0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns=[x for x in df_train.dtypes.index if df_train.dtypes[x]=='object']\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b5cdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.pivot_table(index='Outlet_Type',values='Item_Outlet_Sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7153e3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_columns:\n",
    "    print('Frequency of categories for variable')\n",
    "    print(df_train[col].value_counts())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27b0f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder=LabelEncoder()\n",
    "for col in df_train.columns:\n",
    "    df_train[col] = labelencoder.fit_transform(df_train[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8bd10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.get_dummies(df_train, columns=['Item_Fat_Content',\n",
    " 'Item_Type',\n",
    " 'Outlet_Size',\n",
    " 'Outlet_Location_Type',\n",
    " 'Outlet_Type'],drop_first=False)\n",
    "\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ef9711",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ea1022",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_train[['Item_Weight', 'Item_Visibility', 'Item_MRP',\n",
    "       'Outlet_Establishment_Year',\n",
    "       'Item_Fat_Content_0', 'Item_Fat_Content_1', 'Item_Type_0',\n",
    "       'Item_Type_1', 'Item_Type_2', 'Item_Type_3', 'Item_Type_4',\n",
    "       'Item_Type_5', 'Item_Type_6', 'Item_Type_7', 'Item_Type_8',\n",
    "       'Item_Type_9', 'Item_Type_10', 'Item_Type_11', 'Item_Type_12',\n",
    "       'Item_Type_13', 'Item_Type_14', 'Item_Type_15', 'Outlet_Size_0',\n",
    "       'Outlet_Size_1', 'Outlet_Size_2', 'Outlet_Location_Type_0',\n",
    "       'Outlet_Location_Type_1', 'Outlet_Location_Type_2', 'Outlet_Type_0',\n",
    "       'Outlet_Type_1', 'Outlet_Type_2', 'Outlet_Type_3', 'Item_Outlet_Sales', 'Qty_Sold']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ec5da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a7ca04",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns: \n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a82aa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,0:33].values\n",
    "y = df.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85a7fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c99da7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = None)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "print(explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24015883",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ff440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sorted List returned :\")\n",
    "print(sorted(explained_variance,reverse = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1cea9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('dark_background'):\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    \n",
    "    plt.bar(range(33), explained_variance, alpha=0.5, align='center',label='individual explained variance')\n",
    "    plt.ylabel('Explained variance ratio')\n",
    "    plt.xlabel('Principal components')\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06084c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 3)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba686684",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import AdaBoostRegressor,BaggingRegressor,ExtraTreesRegressor,GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82464bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d6714f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeRegressor()\n",
    "tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac16a100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit Random_forest\n",
    "forest = RandomForestRegressor(n_jobs=-1)\n",
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ddca13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit Ada_Boost_Regressor..........\n",
    "Ada_boost = AdaBoostRegressor()\n",
    "Ada_boost.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99552ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit Bagging_Regressor..........\n",
    "Bagging = BaggingRegressor()\n",
    "Bagging.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd4927c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit Extra_tree_regressor........\n",
    "Extra_trees = ExtraTreesRegressor()\n",
    "Extra_trees.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3bc035",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit Gradient_Boosting_Regressor........\n",
    "Gradient_boosting = GradientBoostingRegressor()\n",
    "Gradient_boosting.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd307dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "models= [('lin_reg', lin_reg), ('forest', forest), ('dt', tree),('Ada_boost',Ada_boost),('Bagging',Bagging),('Extra_trees',Extra_trees),('Gradient_boosting',Gradient_boosting)]\n",
    "scoring = ['neg_mean_squared_error', 'neg_mean_absolute_error', 'r2']\n",
    "\n",
    "\n",
    "#for each model I want to test three different scoring metrics. Therefore, results[0] will be lin_reg x MSE, \n",
    "# results[1] lin_reg x MSE and so on until results [8], where we stored dt x r2\n",
    "\n",
    "results= []\n",
    "metric= []\n",
    "for name, model in models:\n",
    "    for i in scoring:\n",
    "        scores = cross_validate(model, X_train, y_train, scoring=i, cv=10, return_train_score=True)\n",
    "          results.append(scores)\n",
    "\n",
    "print(results[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc60da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you change signa and square the Mean Square Error you get the RMSE, which is the most common metric to accuracy\n",
    "LR_RMSE_mean = np.sqrt(-results[0]['test_score'].mean())\n",
    "LR_RMSE_std= results[0]['test_score'].std()\n",
    "# note that also here I changed the sign, as the result is originally a negative number for ease of computation\n",
    "LR_MAE_mean = -results[1]['test_score'].mean()\n",
    "LR_MAE_std= results[1]['test_score'].std()\n",
    "LR_r2_mean = results[2]['test_score'].mean()\n",
    "LR_r2_std = results[2]['test_score'].std()\n",
    "\n",
    "#THIS IS FOR RF\n",
    "RF_RMSE_mean = np.sqrt(-results[3]['test_score'].mean())\n",
    "RF_RMSE_std= results[3]['test_score'].std()\n",
    "RF_MAE_mean = -results[4]['test_score'].mean()\n",
    "RF_MAE_std= results[4]['test_score'].std()\n",
    "RF_r2_mean = results[5]['test_score'].mean()\n",
    "RF_r2_std = results[5]['test_score'].std()\n",
    "\n",
    "#THIS IS FOR DT\n",
    "DT_RMSE_mean = np.sqrt(-results[6]['test_score'].mean())\n",
    "DT_RMSE_std= results[6]['test_score'].std()\n",
    "DT_MAE_mean = -results[7]['test_score'].mean()\n",
    "DT_MAE_std= results[7]['test_score'].std()\n",
    "DT_r2_mean = results[8]['test_score'].mean()\n",
    "DT_r2_std = results[8]['test_score'].std()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#if you change signa and square the Mean Square Error you get the RMSE, which is the most common metric to accuracy\n",
    "\n",
    "ADA_RMSE_mean = np.sqrt(-results[9]['test_score'].mean())\n",
    "ADA_RMSE_std= results[9]['test_score'].std()\n",
    "# note that also here I changed the sign, as the result is originally a negative number for ease of computation\n",
    "ADA_MAE_mean = -results[10]['test_score'].mean()\n",
    "ADA_MAE_std= results[10]['test_score'].std()\n",
    "ADA_r2_mean = results[11]['test_score'].mean()\n",
    "ADA_r2_std = results[11]['test_score'].std()\n",
    "\n",
    "\n",
    "\n",
    "#if you change signa and square the Mean Square Error you get the RMSE, which is the most common metric to accuracy\n",
    "BAGGING_RMSE_mean = np.sqrt(-results[12]['test_score'].mean())\n",
    "BAGGING_RMSE_std= results[12]['test_score'].std()\n",
    "# note that also here I changed the sign, as the result is originally a negative number for ease of computation\n",
    "BAGGING_MAE_mean = -results[13]['test_score'].mean\n",
    "BAGGING_MAE_std= results[13]['test_score'].std()\n",
    "BAGGING_r2_mean = results[14]['test_score'].mean()\n",
    "BAGGING_r2_std = results[14]['test_score'].std()\n",
    "\n",
    "\n",
    "#if you change signa and square the Mean Square Error you get the RMSE, which is the most common metric to accuracy\n",
    "ET_RMSE_mean = np.sqrt(-results[15]['test_score'].mean())\n",
    "ET_RMSE_std= results[15]['test_score'].std()\n",
    "# note that also here I changed the sign, as the result is originally a negative number for ease of computation\n",
    "ET_MAE_mean = -results[16]['test_score'].mean()\n",
    "ET_MAE_std= results[16]['test_score'].std()\n",
    "ET_r2_mean = results[17]['test_score'].mean()\n",
    "ET_r2_std = results[17]['test_score'].std()\n",
    "\n",
    "\n",
    "#if you change signa and square the Mean Square Error you get the RMSE, which is the most common metric to accuracy\n",
    "GB_RMSE_mean = np.sqrt(-results[18]['test_score'].mean())\n",
    "GB_RMSE_std= results[18]['test_score'].std()\n",
    "# note that also here I changed the sign, as the result is originally a negative number for ease of computation\n",
    "GB_MAE_mean = -results[19]['test_score'].mean()\n",
    "GB_MAE_std= results[19]['test_score'].std()\n",
    "GB_r2_mean = results[20]['test_score'].mean()\n",
    "GB_r2_std = results[20]['test_score'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d342d598",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelDF = pd.DataFrame({\n",
    "    'Model'       : ['Linear Regression', 'Random Forest', 'Decision Trees','Ada Boosting','Bagging','Extra trees','Gradient Boosting'],\n",
    "    'RMSE_mean'    : [LR_RMSE_mean, RF_RMSE_mean, DT_RMSE_mean,ADA_RMSE_mean,BAGGING_RMSE_mean,ET_RMSE_mean,GB_RMSE_mean],\n",
    "    'RMSE_std'    : [LR_RMSE_std, RF_RMSE_std, DT_RMSE_std,ADA_RMSE_std,BAGGING_RMSE_std,ET_RMSE_std,GB_RMSE_std],\n",
    "    'MAE_mean'   : [LR_MAE_mean, RF_MAE_mean, DT_MAE_mean,ADA_MAE_mean,BAGGING_MAE_mean,ET_MAE_mean,GB_MAE_mean],\n",
    "    'MAE_std'   : [LR_MAE_std, RF_MAE_std, DT_MAE_std, ADA_MAE_std, BAGGING_MAE_std, ET_MAE_std, GB_MAE_std],\n",
    "    'r2_mean'      : [LR_r2_mean, RF_r2_mean, DT_r2_mean, ADA_r2_mean,BAGGING_r2_mean, ET_r2_mean, GB_r2_mean],\n",
    "    'r2_std'      : [LR_r2_std, RF_r2_std, DT_r2_std, ADA_r2_std,BAGGING_r2_std, ET_r2_std, GB_r2_std],\n",
    "    }, columns = ['Model', 'RMSE_mean', 'RMSE_std', 'MAE_mean', 'MAE_std', 'r2_mean', 'r2_std'])\n",
    "\n",
    "    \n",
    "modelDF.sort_values(by='r2_mean', ascending=False)\n",
    "Model\tRMSE_mean\tRMSE_std\tMAE_mean\tMAE_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4fdf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.factorplot(x= 'Model', y= 'RMSE_mean', data= modelDF, kind='bar',size=6, aspect=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9f3345",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV,StratifiedKFold\n",
    "\n",
    "ETC = ExtraTreesRegressor()\n",
    "gb_param_grid = {'n_estimators' : [100,200,300,400,500],\n",
    "              'max_depth': [4, 8,12,16],\n",
    "              'min_samples_leaf' : [100,150,200,250],\n",
    "              'max_features' : [0.3, 0.1] \n",
    "              }\n",
    "\n",
    "gsETC = GridSearchCV(ETC,param_grid = gb_param_grid, cv=10, n_jobs= -1, verbose = 0)\n",
    "\n",
    "gsETC.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce47c419",
   "metadata": {},
   "outputs": [],
   "source": [
    "ETC_best = gsETC.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bffa7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best score\n",
    "gsETC.best_score_,gsETC.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b11011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model creation\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "ETC = ExtraTreesRegressor(max_depth= 8,max_features = 0.3,min_samples_leaf =  100,n_estimators= 500)\n",
    "ETC.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#predicting the test set\n",
    "y_pred = ETC.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e5e0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(\"MAE:\", metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('MSE:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e06204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9f0621",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
