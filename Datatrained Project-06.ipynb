{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc36c80e",
   "metadata": {},
   "source": [
    "# Customer Churn Analysis\n",
    "Problem Statement:\n",
    "Customer churn is when a company’s customers stop doing business with that company. Businesses are very keen on measuring churn because keeping an existing customer is far less expensive than acquiring a new customer. New business involves working leads through a sales funnel, using marketing and sales budgets to gain additional customers. Existing customers will often have a higher volume of service consumption and can generate additional customer referrals.\n",
    "\n",
    "Customer retention can be achieved with good customer service and products. But the most effective way for a company to prevent attrition of customers is to truly know them. The vast volumes of data collected about customers can be used to build churn prediction models. Knowing who is most likely to defect means that a company can prioritise focused marketing efforts on that subset of their customer base.\n",
    "\n",
    "Preventing customer churn is critically important to the telecommunications sector, as the barriers to entry for switching services are so low. \n",
    "\n",
    "You will examine customer data from IBM Sample Data Sets with the aim of building and comparing several customer churn prediction models. \n",
    "\n",
    "Note: You can find the dataset in the link below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6eaa22df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ff283b23",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\\\\\Users\\\\\\\\win 7\\\\\\\\Desktop\\\\\\\\Datascience\\\\\\\\Customer Churn Analysis.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\WIN7~1\\AppData\\Local\\Temp/ipykernel_588/129189345.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"C:\\\\Users\\\\win 7\\\\Desktop\\\\Datascience\\\\Customer Churn Analysis.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\\\\\Users\\\\\\\\win 7\\\\\\\\Desktop\\\\\\\\Datascience\\\\\\\\Customer Churn Analysis.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\\\Users\\\\win 7\\\\Desktop\\\\Datascience\\\\Customer Churn Analysis.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3686420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking for missing data\n",
    "missing_data=df.isnull()\n",
    "for column in missing_data.columns.values.tolist():\n",
    "    print(column)\n",
    "    print(missing_data[column].value_counts())\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaa5efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3157a3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the summary\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c91dad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at Geography and Gender Distribution against Estimated Salary\n",
    "plt.figure(figsize=(20,20))\n",
    "sns.catplot(x=\"Geography\", y=\"EstimatedSalary\", hue=\"Gender\", kind=\"box\", data=df)\n",
    "plt.title(\"Geography VS Estimated Salary\")\n",
    "plt.xlabel(\"Geography\")\n",
    "plt.ylabel(\"Estimated Salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef95ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at linear relationship between Age and CreditScore\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.regplot(x=\"Age\", y=\"CreditScore\", data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a316bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting and Preparing the Feature Set and Target\n",
    "X = df[[\"CreditScore\", \"Geography\", \"Gender\", \"Age\", \"Tenure\", \"EstimatedSalary\"]].values\n",
    "y=df[[\"Exited\"]]\n",
    "X[0:5], y[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc2fa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing categorical variables\n",
    "from sklearn import preprocessing\n",
    "geography=preprocessing.LabelEncoder()\n",
    "geography.fit([\"France\", \"Spain\", \"Germany\"])\n",
    "X[:,1]=geography.transform(X[:,1])\n",
    "\n",
    "gender = preprocessing.LabelEncoder()\n",
    "gender.fit([\"Female\", \"Male\"])\n",
    "X[:,2]=gender.transform(X[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10e1ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_trainset, X_testset, y_trainset, y_testset = train_test_split(X, y, test_size=0.2, random_state=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29775852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model using DecisionTree Classifier and fit training data\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt_model = DecisionTreeClassifier()\n",
    "dt_model.fit(X_trainset, y_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f3a36eeb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dt_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\WIN7~1\\AppData\\Local\\Temp/ipykernel_588/3945698291.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# create prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdt_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdt_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_testset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdt_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dt_model' is not defined"
     ]
    }
   ],
   "source": [
    "# create prediction\n",
    "dt_pred = dt_model.predict(X_testset)\n",
    "dt_pred[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08a65ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the prediction model\n",
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_testset, dt_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60951bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Random Forest Decision Tree model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100)\n",
    "rf_model.fit(X_trainset, y_trainset.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4911e9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create prediction using rf_model\n",
    "rf_pred = rf_model.predict(X_testset)\n",
    "rf_pred[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec39427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "metrics.accuracy_score(y_testset, rf_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ba0297",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\\\Users\\\\win 7\\\\Desktop\\\\Datascience\\\\Customer Churn Analysis.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9c25bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loop to see unique values\n",
    "for column in df.columns.values.tolist():\n",
    "    print(column)\n",
    "    print(df[column].unique())\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bccb574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection\n",
    "X = df[[\"account length\", \"international plan\", \"total day charge\", \"total night charge\", \"total intl charge\", \"customer service calls\", \"state\"]]\n",
    "# target selection\n",
    "y =df[\"churn\"]\n",
    "# review feature set\n",
    "X[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dcddf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update state with one hot coding\n",
    "X=pd.get_dummies(X, columns=[\"state\"])\n",
    "# make sure i am using feature set values \n",
    "X=X.values\n",
    "# preprocess to update str variables to numerical variables\n",
    "from sklearn import preprocessing\n",
    "international_plan=preprocessing.LabelEncoder()\n",
    "international_plan.fit([\"no\", \"yes\"])\n",
    "X[:,1] = international_plan.transform(X[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "392ed4f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\WIN7~1\\AppData\\Local\\Temp/ipykernel_588/533789464.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# create training and testing set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX_trainset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_testset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_trainset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_testset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# create training and testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_trainset, X_testset, y_trainset, y_testset = train_test_split(X, y, test_size=0.2, random_state=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd4e912",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model using random forest classifier and fit the training set\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100)\n",
    "rf_model.fit(X_trainset, y_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5cc6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create prediction using the model\n",
    "rf_pred = rf_model.predict(X_testset)\n",
    "rf_pred[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f80395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the accuracy score (using two methods)\n",
    "from sklearn import metrics\n",
    "rf_model.score(X_testset, y_testset)\n",
    "metrics.accuracy_score(y_testset, rf_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ad0d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrics to find precision and recall\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_testset, rf_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0b331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the precision score\n",
    "from sklearn.metrics import precision_score\n",
    "precision_score(y_testset, rf_pred)\n",
    "\n",
    "# Looking at the recall score\n",
    "from sklearn.metrics import recall_score\n",
    "recall_score(y_testset, rf_pred)\n",
    "\n",
    "# find probability for each prediction\n",
    "prob=rf_model.predict_proba(X_testset)[:,1]\n",
    "\n",
    "# look at ROC curve, which gives us the false and true positive predictions\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds=roc_curve(y_testset, prob)\n",
    "plt.plot(fpr, tpr)\n",
    "\n",
    "# Looking at the area under the curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "auc=roc_auc_score(y_testset, prob)\n",
    "auc\n",
    "\n",
    "#looking at the f1_score\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_testset, rf_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c8c8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the importance of each feature\n",
    "importances=rf_model.feature_importances_\n",
    "\n",
    "# visualize to see the feature importance\n",
    "indices=np.argsort(importances)[::-1]\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(range(X.shape[1]), importances[indices])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8385720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the svm model and fitting training set\n",
    "# make sure to update probability to True for proabbility evaluation\n",
    "from sklearn.svm import SVC\n",
    "svc_model=SVC(probability=True)\n",
    "svc_model.fit(X_trainset, y_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787d8121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the accuracy score\n",
    "svc_model.score(X_testset, y_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06525f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_testset, svc_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9330fc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#precision score for svm\n",
    "precision_score(y_testset, svc_pred)\n",
    "# recall score for svm\n",
    "recall_score(y_testset, svc_pred)\n",
    "# probability for each prediction\n",
    "prob_2=svc_model.predict_proba(X_testset)[:,1]\n",
    "# look at ROC curve\n",
    "fpr, tpr, thresholds=roc_curve(y_testset, prob_2)\n",
    "plt.plot(fpr, tpr)\n",
    "# area under the curve\n",
    "auc=roc_auc_score(y_testset, prob)\n",
    "auc\n",
    "# find ideal degree for SVM model\n",
    "param_grid_2={'degree': np.arange(1,50)}\n",
    "svc_cv=GridSearchCV(SVC(), param_grid_2)\n",
    "svc_cv.fit(X,y)\n",
    "svc_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1078e78f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c964ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5a3b6c0",
   "metadata": {},
   "source": [
    "# Rainfall Prediction - Weather Forecasting\n",
    "Problem Statement:\n",
    "Rain Prediction –Weather forecasting\n",
    "\n",
    "Weather forecasting is the application of science and technology to predict the conditions of the atmosphere for a given location and time. Weather forecasts are made by collecting quantitative data about the current state of the atmosphere at a given place and using meteorology to project how the atmosphere will change.\n",
    "\n",
    "Rain Dataset is to predict whether or not it will rain tomorrow. The Dataset contains about 10 years of daily weather observations of different locations in Australia. Here, predict two things:\n",
    " \n",
    "1. Problem Statement: \n",
    "a) Design a predictive model with the use of machine learning algorithms to forecast whether or not it will rain tomorrow.\n",
    "\n",
    "b)  Design a predictive model with the use of machine learning algorithms to predict how much rainfall could be there.\n",
    "\n",
    "\n",
    "Dataset Description:\n",
    "\n",
    "Number of columns: 23\n",
    "\n",
    "\n",
    "Date  - The date of observation\n",
    "\n",
    "Location  -The common name of the location of the weather station\n",
    "\n",
    "MinTemp  -The minimum temperature in degrees celsius\n",
    "\n",
    "MaxTemp -The maximum temperature in degrees celsius\n",
    "\n",
    "Rainfall  -The amount of rainfall recorded for the day in mm\n",
    "\n",
    "Evaporation  -The so-called Class A pan evaporation (mm) in the 24 hours to 9am\n",
    "\n",
    "Sunshine  -The number of hours of bright sunshine in the day.\n",
    "\n",
    "WindGustDi r- The direction of the strongest wind gust in the 24 hours to midnight\n",
    "\n",
    "WindGustSpeed -The speed (km/h) of the strongest wind gust in the 24 hours to midnight\n",
    "\n",
    "WindDir9am -Direction of the wind at 9am\n",
    "\n",
    "WindDir3pm -Direction of the wind at 3pm\n",
    "\n",
    "WindSpeed9am -Wind speed (km/hr) averaged over 10 minutes prior to 9am\n",
    "\n",
    "WindSpeed3pm -Wind speed (km/hr) averaged over 10 minutes prior to 3pm\n",
    "\n",
    "Humidity9am -Humidity (percent) at 9am\n",
    "\n",
    "Humidity3pm -Humidity (percent) at 3pm\n",
    "\n",
    "Pressure9am -Atmospheric pressure (hpa) reduced to mean sea level at 9am\n",
    "\n",
    "Pressure3pm -Atmospheric pressure (hpa) reduced to mean sea level at 3pm\n",
    "\n",
    "Cloud9am - Fraction of sky obscured by cloud at 9am. \n",
    "\n",
    "Cloud3pm -Fraction of sky obscured by cloud \n",
    "\n",
    "Temp9am-Temperature (degrees C) at 9am\n",
    "\n",
    "Temp3pm -Temperature (degrees C) at 3pm\n",
    "\n",
    "RainToday -Boolean: 1 if precipitation (mm) in the 24 hours to 9am exceeds 1mm, otherwise 0\n",
    "\n",
    "RainTomorrow -The amount of next day rain in mm. Used to create response variable . A kind of measure of the \"risk\".\n",
    "\n",
    "\n",
    "Dataset available below\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6220e279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import folium\n",
    "import imageio\n",
    "from tqdm import tqdm_notebook\n",
    "from folium.plugins import MarkerCluster\n",
    "import geoplot as gplt\n",
    "import geopandas as gpd\n",
    "import geoplot.crs as gcrs\n",
    "import imageio\n",
    "import mapclassify as mc\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "import scipy\n",
    "from itertools import product\n",
    "import seaborn as sns\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.stattools import pacf\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['font.family'] = 'sans-serif' \n",
    "plt.rcParams['font.serif'] = 'Ubuntu' \n",
    "plt.rcParams['font.monospace'] = 'Ubuntu Mono' \n",
    "plt.rcParams['font.size'] = 14 \n",
    "plt.rcParams['axes.labelsize'] = 12 \n",
    "plt.rcParams['axes.labelweight'] = 'bold' \n",
    "plt.rcParams['axes.titlesize'] = 12 \n",
    "plt.rcParams['xtick.labelsize'] = 12 \n",
    "plt.rcParams['ytick.labelsize'] = 12 \n",
    "plt.rcParams['legend.fontsize'] = 12 \n",
    "plt.rcParams['figure.titlesize'] = 12 \n",
    "plt.rcParams['image.cmap'] = 'jet' \n",
    "plt.rcParams['image.interpolation'] = 'none' \n",
    "plt.rcParams['figure.figsize'] = (12, 10) \n",
    "plt.rcParams['axes.grid']=True\n",
    "plt.rcParams['lines.linewidth'] = 2 \n",
    "plt.rcParams['lines.markersize'] = 8\n",
    "colors = ['xkcd:pale orange', 'xkcd:sea blue', 'xkcd:pale red', 'xkcd:sage green', 'xkcd:terra cotta', 'xkcd:dull purple', 'xkcd:teal', 'xkcd: goldenrod', 'xkcd:cadet blue',\n",
    "'xkcd:scarlet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7a7f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"C:\\\\Users\\\\win 7\\\\Desktop\\Datascience\\\\Rainfall Predication & Wheather Predication.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e233110",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_data = data.drop_duplicates(['City'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1d02b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAT = []\n",
    "LONG = []\n",
    "for city in city_data.City.tolist():\n",
    "    locator = Nominatim(user_agent=\"myGeocoder\")\n",
    "    location = locator.geocode(city)\n",
    "    LAT.append(location.latitude)\n",
    "    LONG.append(location.longitude)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec206bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "world_map= folium.Map()\n",
    "geolocator = Nominatim(user_agent=\"Piero\")\n",
    "marker_cluster = MarkerCluster().add_to(world_map)\n",
    "\n",
    "for i in range(len(city_data)):\n",
    "        lat = city_data.iloc[i]['Latitude']\n",
    "        long = city_data.iloc[i]['Longitude']\n",
    "        radius=5\n",
    "        folium.CircleMarker(location = [lat, long], radius=radius,fill =True, color='darkred',fill_color='darkred').add_to(marker_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adce6e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "explodes = (0,0.3)\n",
    "plt.pie(data[data['City']=='Chicago'].AverageTemperature.isna().value_counts(),explode=explodes,startangle=0,colors=['firebrick','indianred'],\n",
    "   labels=['Non NaN elements','NaN elements'], textprops={'fontsize': 20})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedad392",
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_data = data[data['City']=='Chicago']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa315f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_data['AverageTemperature']=chicago_data.AverageTemperature.fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df7340f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_data['AverageTemperatureUncertainty']=chicago_data.AverageTemperatureUncertainty.fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326881d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_data = chicago_data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdee2c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_data = chicago_data.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a130a7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_data.dt = pd.to_datetime(chicago_data.dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d1fa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR = []\n",
    "MONTH = []\n",
    "DAY = []\n",
    "WEEKDAY = []\n",
    "for i in range(len(chicago_data)):\n",
    "    WEEKDAY.append(chicago_data.dt[i].weekday())\n",
    "    DAY.append(chicago_data.dt[i].day)\n",
    "    MONTH.append(chicago_data.dt[i].month)\n",
    "    YEAR.append(chicago_data.dt[i].year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb31691",
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_data['Year'] = YEAR\n",
    "chicago_data['Month'] = MONTH\n",
    "chicago_data['Day'] = DAY \n",
    "chicago_data['Weekday'] = WEEKDAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44649a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_year_index = []\n",
    "change_year = []\n",
    "year_list = chicago_data['Year'].tolist()\n",
    "for y in range(0,len(year_list)-1):\n",
    "    if year_list[y]!=year_list[y+1]:\n",
    "        change_year.append(year_list[y+1])\n",
    "        change_year_index.append(y+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a51d2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_data.loc[change_year_index].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5633e2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ticks_year_list=np.linspace(min(year_list),max(year_list),10).astype(int)\n",
    "change_year_index = np.array(change_year_index)\n",
    "x_ticks_year_index = []\n",
    "for i in range(1,len(x_ticks_year_list)):\n",
    "    x_ticks_year_index.append(change_year_index[np.where(np.array(change_year)==x_ticks_year_list[i])][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c0f0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=chicago_data.index,y=chicago_data.AverageTemperature,s=25,color='firebrick')\n",
    "plt.xticks(x_ticks_year_index,x_ticks_year_list)\n",
    "plt.title('Temperature vs Year Scatter plot',color='firebrick',fontsize=40)\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Average Temperature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd59684",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.suptitle('Plotting 4 decades',fontsize=40,color='firebrick')\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.title('Starting year: 1800, Ending Year: 1810',fontsize=15)\n",
    "plot_timeseries(1800,1810)\n",
    "plt.subplot(2,2,2)\n",
    "plt.title('Starting year: 1900, Ending Year: 1910',fontsize=15)\n",
    "plot_timeseries(1900,1910)\n",
    "plt.subplot(2,2,3)\n",
    "plt.title('Starting year: 1950, Ending Year: 1960',fontsize=15)\n",
    "plot_timeseries(1900,1910)\n",
    "plt.subplot(2,2,4)\n",
    "plt.title('Starting year: 2000, Ending Year: 2010',fontsize=15)\n",
    "plot_timeseries(1900,1910)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f869c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "fig = sm.graphics.tsa.plot_acf(chicago_data.AverageTemperature, ax=ax1,color ='firebrick')\n",
    "ax2 = fig.add_subplot(212)\n",
    "fig = sm.graphics.tsa.plot_pacf(chicago_data.AverageTemperature, ax=ax2,color='firebrick')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7787a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = adfuller(chicago_data.AverageTemperature)\n",
    "print('ADF Statistic on the entire dataset: {}'.format(result[0]))\n",
    "print('p-value: {}'.format(result[1]))\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "    print('\\t{}: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c79476",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = adfuller(chicago_data.AverageTemperature[0:120])\n",
    "print('ADF Statistic on the first decade: {}'.format(result[0]))\n",
    "print('p-value: {}'.format(result[1]))\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "    print('\\t{}: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e9e3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('The dataset used for prediction', fontsize=30,color='firebrick')\n",
    "plot_timeseries(1992,2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbfce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = get_timeseries(1992,2013)\n",
    "N = len(temp.AverageTemperature)\n",
    "split = 0.95\n",
    "training_size = round(split*N)\n",
    "test_size = round((1-split)*N)\n",
    "series = temp.AverageTemperature[:training_size]\n",
    "date = temp.dt[:training_size]\n",
    "test_series = temp.AverageTemperature[len(date)-1:len(temp)]\n",
    "test_date = temp.dt[len(date)-1:len(temp)]\n",
    "#test_date = test_date.reset_index().dt\n",
    "#test_series = test_series.reset_index().AverageTemperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6efb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_from_data(series,date,label='Training Set')\n",
    "plot_from_data(test_series,test_date,'navy',with_ticks=False,label='Test Set')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc7eae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_ARIMA(order_list, exog):\n",
    "    \"\"\"\n",
    "        Return dataframe with parameters and corresponding AIC\n",
    "        \n",
    "        order_list - list with (p, d, q) tuples\n",
    "        exog - the exogenous variable\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for order in tqdm_notebook(order_list):\n",
    "        #try: \n",
    "        model = SARIMAX(exog, order=order).fit(disp=-1)\n",
    "    #except:\n",
    "    #        continue\n",
    "            \n",
    "        aic = model.aic\n",
    "        results.append([order, model.aic])\n",
    "    #print(results)\n",
    "    result_df = pd.DataFrame(results)\n",
    "    result_df.columns = ['(p, d, q)', 'AIC']\n",
    "    #Sort in ascending order, lower AIC is better\n",
    "    result_df = result_df.sort_values(by='AIC', ascending=True).reset_index(drop=True)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389a4f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = range(0, 10, 1)\n",
    "d = 0\n",
    "qs = range(0, 10, 1)\n",
    "\n",
    "# Create a list with all possible combination of parameters\n",
    "parameters = product(ps, qs)\n",
    "parameters_list = list(parameters)\n",
    "\n",
    "order_list = []\n",
    "\n",
    "for each in parameters_list:\n",
    "    each = list(each)\n",
    "    each.insert(1, d)\n",
    "    each = tuple(each)\n",
    "    order_list.append(each)\n",
    "    \n",
    "result_d_0 = optimize_ARIMA(order_list, exog = series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727e0341",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_d_0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a5e068",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = range(0, 10, 1)\n",
    "d = 1\n",
    "qs = range(0, 10, 1)\n",
    "\n",
    "# Create a list with all possible combination of parameters\n",
    "parameters = product(ps, qs)\n",
    "parameters_list = list(parameters)\n",
    "\n",
    "order_list = []\n",
    "\n",
    "for each in parameters_list:\n",
    "    each = list(each)\n",
    "    each.insert(1, d)\n",
    "    each = tuple(each)\n",
    "    order_list.append(each)\n",
    "    \n",
    "result_d_1 = optimize_ARIMA(order_list, exog = series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e238a192",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_d_1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edcf4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = result_d_0.append(result_d_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194bea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = final_result.sort_values(by='AIC', ascending=True).reset_index(drop=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a97544",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6bde0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_params_0 = best_models[best_models.columns[0]][0]\n",
    "best_model_params_1 = best_models[best_models.columns[0]][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e80a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_0 = SARIMAX(series, order=best_model_params_0).fit()\n",
    "print(best_model_0.summary())\n",
    "best_model_1 = SARIMAX(series, order=best_model_params_1).fit()\n",
    "print(best_model_1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66471297",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_0.plot_diagnostics(figsize=(15,12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f807ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_1.plot_diagnostics(figsize=(15,12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc932776",
   "metadata": {},
   "outputs": [],
   "source": [
    "fore_l= test_size-1\n",
    "forecast = best_model_0.get_prediction(start=training_size, end=training_size+fore_l)\n",
    "forec = forecast.predicted_mean\n",
    "ci = forecast.conf_int(alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5456507",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_test=chicago_data.loc[test_date[1:].index.tolist()].AverageTemperatureUncertainty\n",
    "index_test = test_date[1:].index.tolist()\n",
    "test_set = test_series[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533e21b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_test = test_set-error_test\n",
    "upper_test = test_set+error_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566d7ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,8), dpi=300)\n",
    "x0 = chicago_data.AverageTemperature.index[0:training_size]\n",
    "x1=chicago_data.AverageTemperature.index[training_size:training_size+fore_l+1]\n",
    "#ax.fill_between(forec, ci['lower Load'], ci['upper Load'])\n",
    "plt.plot(x0, chicago_data.AverageTemperature[0:training_size],'k', label = 'Average Temperature')\n",
    "\n",
    "plt.plot(chicago_data.AverageTemperature[training_size:training_size+fore_l], '.k', label = 'Actual')\n",
    "\n",
    "#forec = pd.DataFrame(forec, columns=['f'], index = x1)\n",
    "#forec.f.plot(ax=ax,color = 'Darkorange',label = 'Forecast (d = 2)')\n",
    "#ax.fill_between(x1, ci['lower AverageTemperature'], ci['upper AverageTemperature'],alpha=0.2, label = 'Confidence inerval (95%)',color='grey')\n",
    "\n",
    "forec = pd.DataFrame(s_forec, columns=['f'], index = x1)\n",
    "forec.f.plot(ax=ax,color = 'firebrick',label = 'Forecast  (2,1,5) model')\n",
    "ax.fill_between(x1, ci['lower AverageTemperature'], ci['upper AverageTemperature'],alpha=0.2, label = 'Confidence inerval (95%)',color='grey')\n",
    "\n",
    "\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.xlim(120,265)\n",
    "plt.xlabel('Index Datapoint')\n",
    "plt.ylabel('Temperature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e6eee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(forec)\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.subplot(2,1,1)\n",
    "plt.fill_between(x1, lower_test, upper_test,alpha=0.2, label = 'Test set error range',color='navy')\n",
    "plt.plot(test_set,marker='.',label=\"Actual\",color='navy')\n",
    "plt.plot(forec,marker='d',label=\"Forecast\",color='firebrick')\n",
    "plt.xlabel('Index Datapoint')\n",
    "plt.ylabel('Temperature')\n",
    "#plt.fill_between(x1, s_ci['lower AverageTemperature'], s_ci['upper AverageTemperature'],alpha=0.3, label = 'Confidence inerval (95%)',color='firebrick')\n",
    "plt.legend()\n",
    "plt.subplot(2,1,2)\n",
    "#plt.fill_between(x1, lower_test, upper_test,alpha=0.2, label = 'Test set error range',color='navy')\n",
    "plt.plot(test_set,marker='.',label=\"Actual\",color='navy')\n",
    "plt.plot(s_forec,marker='d',label=\"Forecast\",color='firebrick')\n",
    "plt.fill_between(x1, ci['lower AverageTemperature'], ci['upper AverageTemperature'],alpha=0.3, label = 'Confidence inerval (95%)',color='firebrick')\n",
    "plt.legend()\n",
    "plt.xlabel('Index Datapoint')\n",
    "plt.ylabel('Temperature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bad8b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.fill_between(np.arange(0,len(test_set),1), lower_test, upper_test,alpha=0.2, label = 'Test set error range',color='navy')\n",
    "plot_from_data(test_set,test_date,c='navy',label='Actual')\n",
    "plot_from_data(forec['f'],test_date,c='firebrick',label='Forecast')\n",
    "plt.legend(loc=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951bc935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf35d82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d30d344",
   "metadata": {},
   "source": [
    "# Insurance Claims- Fraud Detection\n",
    "Problem Statement:\n",
    "Business case:\n",
    "Insurance fraud is a huge problem in the industry. It's difficult to identify fraud claims. Machine Learning is in a unique position to help the Auto Insurance industry with this problem.\n",
    "\n",
    "In this project, you are provided a dataset which has the details of the insurance policy along with the customer details. It also has the details of the accident on the basis of which the claims have been made. \n",
    "\n",
    "In this example, you will be working with some auto insurance data to demonstrate how you can create a predictive model that predicts if an insurance claim is fraudulent or not. \n",
    "\n",
    " \n",
    "\n",
    "Note: Use the link below to reach to your dataset. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4ed4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "from pylab import rcParams\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e1b391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "from pylab import rcParams\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4562de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load & view raw data\n",
    "df = pd.read_csv('C:\\\\Users\\\\win 7\\\\Desktop\\\\Datascience\\\\Insurance Claims Fraud Detection.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def239f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734048b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451f6ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df7af08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d42e6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "ax = sns.countplot(x='fraud_reported', data=df, hue='fraud_reported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182e500d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fraud_reported'].value_counts() # Count number of frauds vs non-frauds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a63f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['incident_state'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3371710",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = df.groupby('incident_state').fraud_reported.count().plot.bar(ylim=0)\n",
    "ax.set_ylabel('Fraud reported')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6b61c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "fig = plt.figure(figsize=(18,6))\n",
    "ax = df.groupby('incident_date').total_claim_amount.count().plot.bar(ylim=0)\n",
    "ax.set_ylabel('Claim amount ($)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f657ad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = df.groupby('policy_state').fraud_reported.count().plot.bar(ylim=0)\n",
    "ax.set_ylabel('Fraud reported')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42588c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = df.groupby('incident_type').fraud_reported.count().plot.bar(ylim=0)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=20, ha=\"right\")\n",
    "ax.set_ylabel('Fraud reported')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b86de8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = sns.countplot(x='incident_state', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a1a29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = sns.countplot(y = 'insured_education_level', data=df) \n",
    "ax.set_ylabel('policy_annual_premium')\n",
    "plt.show()\n",
    "\n",
    "# # Breakdown of Average Vehicle claim by insured's education level, grouped by fraud reported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cb6c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = (df['insured_sex'].value_counts()*100.0 /len(df))\\\n",
    ".plot.pie(autopct='%.1f%%', labels = ['Male', 'Female'], fontsize=12)                                                                           \n",
    "ax.set_title('% Gender')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8d716b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = (df['insured_relationship'].value_counts()*100.0 /len(df))\\\n",
    ".plot.pie(autopct='%.1f%%', labels = ['husband', 'wife', 'own-child', 'unmarried', 'other-relative', 'not-in-family'],\n",
    "         fontsize=12)                                                                           \n",
    "ax.set_title('% Relationship')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372cbdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = (df['incident_type'].value_counts()*100.0 /len(df))\\\n",
    ".plot.pie(autopct='%.1f%%', labels = ['Parked Car', 'Single Vehile Collision', 'Multi-vehicle Collision', 'Vehicle Theft'],\n",
    "         fontsize=12) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12e5ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = (df['authorities_contacted'].value_counts()*100.0 /len(df))\\\n",
    ".plot.pie(autopct='%.1f%%', labels = ['Police', 'Fire', 'Other', 'None', 'Ambulance'],\n",
    "         fontsize=12) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5270e70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = sns.countplot(x='auto_make', data=df)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d946c68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = (df['incident_severity'].value_counts()*100.0 /len(df))\\\n",
    ".plot.pie(autopct='%.1f%%', labels = ['Major Damage', 'Total Loss', 'Minor Damage', 'Trivial Damage'],\n",
    "         fontsize=12)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e88691",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = sns.countplot(x='insured_hobbies', data=df)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23ee03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"insured_occupation\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fa4310",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax= df.groupby('auto_make').vehicle_claim.count().plot.bar(ylim=0)\n",
    "ax.set_ylabel('Vehicle claim')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4655de",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax= df.groupby('insured_hobbies').total_claim_amount.count().plot.bar(ylim=0)\n",
    "ax.set_ylabel('Total claim amount')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b831732",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fraud_reported'].replace(to_replace='Y', value=1, inplace=True)\n",
    "df['fraud_reported'].replace(to_replace='N',  value=0, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03133ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['insured_zip']] = df[['insured_zip']].astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37646929",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfc3927",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.auto_year.value_counts()  # check the spread of years to decide on further action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409d8220",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vehicle_age'] = 2018 - df['auto_year'] # Deriving the age of the vehicle based on the year value \n",
    "df['vehicle_age'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f568572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [-1, 3, 6, 9, 12, 17, 20, 24]  # Factorize according to the time period of the day.\n",
    "names = [\"past_midnight\", \"early_morning\", \"morning\", 'fore-noon', 'afternoon', 'evening', 'night']\n",
    "df['incident_period_of_day'] = pd.cut(df.incident_hour_of_the_day, bins, labels=names).astype(object)\n",
    "df[['incident_hour_of_the_day', 'incident_period_of_day']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5694748f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check on categorical variables:\n",
    "df.select_dtypes(include=['object']).columns  # checking categorcial columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1f4b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping unimportant columns\n",
    "\n",
    "df = df.drop(columns = [\n",
    "    'policy_number', \n",
    "    'insured_zip', \n",
    "    'policy_bind_date', \n",
    "    'incident_date', \n",
    "    'incident_location', \n",
    "    '_c39', \n",
    "    'auto_year', \n",
    "    'incident_hour_of_the_day'])\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d995b933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    0\n",
      "Restaurant ID,Restaurant Name,Country Code,City...  0\n"
     ]
    }
   ],
   "source": [
    "# identify variables with '?' values\n",
    "unknowns = {}\n",
    "for i in list(df.columns):\n",
    "    if (df[i]).dtype == object:\n",
    "        j = np.sum(df[i] == \"?\")\n",
    "        unknowns[i] = j\n",
    "unknowns = pd.DataFrame.from_dict(unknowns, orient = 'index')\n",
    "print(unknowns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "279ef55e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'collision_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\WIN7~1\\AppData\\Local\\Temp/ipykernel_588/2293704817.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollision_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5485\u001b[0m         ):\n\u001b[0;32m   5486\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5487\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5489\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'collision_type'"
     ]
    }
   ],
   "source": [
    "df.collision_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e0a6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax= df.groupby('collision_type').police_report_available.count().plot.bar(ylim=0)\n",
    "ax.set_ylabel('Police report')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=10, ha=\"right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9c1abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.property_damage.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89af128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax= df.groupby('property_damage').police_report_available.count().plot.bar(ylim=0)\n",
    "ax.set_ylabel('Police report')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=10, ha=\"right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f29a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.police_report_available.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ad0428",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7755ee1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df._get_numeric_data().head()  # Checking numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28311a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df._get_numeric_data().columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e854be3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(include=['object']).columns  # checking categorcial columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39371e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(df[[\n",
    "    'policy_state', \n",
    "    'policy_csl', \n",
    "    'insured_sex', \n",
    "    'insured_education_level',\n",
    "    'insured_occupation', \n",
    "    'insured_hobbies', \n",
    "    'insured_relationship',\n",
    "    'incident_type', \n",
    "    'incident_severity',\n",
    "    'authorities_contacted', \n",
    "    'incident_state', \n",
    "    'incident_city',\n",
    "    'auto_make', \n",
    "    'auto_model',\n",
    "    'incident_period_of_day']])\n",
    "\n",
    "dummies = dummies.join(df[[\n",
    "    'collision_type', \n",
    "    'property_damage', \n",
    "    'police_report_available', \n",
    "    \"fraud_reported\"]])\n",
    "\n",
    "dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859f9a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dummies.iloc[:, 0:-1]\n",
    "y = dummies.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81ef7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f905f547",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d4aab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474924b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "X['collision_en'] = LabelEncoder().fit_transform(dummies['collision_type'])\n",
    "X[['collision_type', 'collision_en']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f1d11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['property_damage'].replace(to_replace='YES', value=1, inplace=True)\n",
    "X['property_damage'].replace(to_replace='NO', value=0, inplace=True)\n",
    "X['property_damage'].replace(to_replace='?', value=0, inplace=True)\n",
    "X['police_report_available'].replace(to_replace='YES', value=1, inplace=True)\n",
    "X['police_report_available'].replace(to_replace='NO', value=0, inplace=True)\n",
    "X['police_report_available'].replace(to_replace='?', value=0, inplace=True)\n",
    "\n",
    "X.head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d925e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(columns = ['collision_type'])\n",
    "X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d028112",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([X, df._get_numeric_data()], axis=1)  # joining numeric columns\n",
    "X.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bc4f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c3d371",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(columns = ['fraud_reported'])\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fe6c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# evaluate an LDA model on the dataset using k-fold cross validation\n",
    "model = LinearDiscriminantAnalysis()\n",
    "kfold = KFold(n_splits=5, random_state=7)\n",
    "result = cross_val_score(model, X, y, cv=kfold, scoring='accuracy')\n",
    "print(result.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f0cab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=1234)\n",
    "print('length of X_train and X_test: ', len(X_train), len(X_test))\n",
    "print('length of y_train and y_test: ', len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e770e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, classification_report, cohen_kappa_score\n",
    "from sklearn import metrics \n",
    "\n",
    "# Baseline Random forest based Model\n",
    "rfc = RandomForestClassifier(criterion = 'gini', n_estimators=1000, verbose=1, n_jobs = -1, \n",
    "                             class_weight = 'balanced', max_features = 'auto')\n",
    "rfcg = rfc.fit(X_train,y_train) # fit on training data\n",
    "predictions = rfcg.predict(X_test)\n",
    "\n",
    "print('Baseline: N_features: ', len(list(X.columns)))\n",
    "print('Baseline: Accuracy: ', round(accuracy_score(y_test, predictions)*100, 2))\n",
    "print( 'Cohen Kappa: '+ str(np.round(cohen_kappa_score(y_test, predictions),3)))\n",
    "print('Baseline: Recall: ', round(recall_score(y_test, predictions)*100, 2))\n",
    "print('\\n Classification Report:\\n', classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6075cb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d28e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import itertools\n",
    "\n",
    "#Evaluation of Model - Confusion Matrix Plot\n",
    "def plot_confusion_matrix(cm, classes, title ='Confusion matrix', normalize=False, cmap = plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    print('Confusion matrix')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, predictions)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['Fraud reported_Y','Fraud_reported_N'],\n",
    "                      title='Random Forest-Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada13ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a Histogram plot for anomaly detection\n",
    "df.plot(kind='hist')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbc33bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum and maximum premium \n",
    "print('Minimum premimum ' + str(df['policy_annual_premium'].min()))\n",
    "print('Maximum premium ' + str(df['policy_annual_premium'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0608e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum and maximum age of vehicle\n",
    "print('Vehicle age-minimum ' + str(df['vehicle_age'].min()))\n",
    "print('Vehicle Age-maximum ' + str(df['vehicle_age'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0292b34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f89845b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "logreg2= LogisticRegressionCV(solver='lbfgs', cv=10)\n",
    "knn = KNeighborsClassifier(5)\n",
    "svcl = SVC()\n",
    "adb = AdaBoostClassifier()\n",
    "dtclf = DecisionTreeClassifier(max_depth=5)\n",
    "rfclf = RandomForestClassifier()\n",
    "\n",
    "# prepare configuration for cross validation test harness\n",
    "seed = 7\n",
    "# prepare models\n",
    "models = []\n",
    "models.append(('LR', LogisticRegressionCV(solver='lbfgs', max_iter=5000, cv=10)))\n",
    "models.append(('XGB', XGBClassifier()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('DT', DecisionTreeClassifier()))\n",
    "models.append(('SVM', SVC(gamma='auto')))\n",
    "models.append(('RF', RandomForestClassifier(n_estimators=100)))\n",
    "models.append(('ADA', AdaBoostClassifier(n_estimators=100)))\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(model, X_train_scaled, y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad0e472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f129ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1943f249",
   "metadata": {},
   "source": [
    "# Zomato Restaurant Project\n",
    "Problem Statement:\n",
    "Zomato Data Analysis is one of the most useful analysis for foodies who want to taste the best\n",
    "cuisines of every part of the world which lies in their budget. This analysis is also for those who\n",
    "want to find the value for money restaurants in various parts of the country for the cuisines.\n",
    "Additionally, this analysis caters the needs of people who are striving to get the best cuisine of\n",
    "the country and which locality of that country serves that cuisines with maximum number of\n",
    "restaurants.\n",
    "\n",
    "Data Storage:\n",
    "This problem statement contains two datasets- Zomato.csv and country_code.csv.\n",
    "Country_code.csv contains two variables:\n",
    " Country code\n",
    " Country name\n",
    "The collected data has been stored in the Comma Separated Value file Zomato.csv. Each\n",
    "restaurant in the dataset is uniquely identified by its Restaurant Id. Every Restaurant contains the\n",
    "following variables:\n",
    "• Restaurant Id: Unique id of every restaurant across various cities of the world\n",
    "• Restaurant Name: Name of the restaurant\n",
    "• Country Code: Country in which restaurant is located\n",
    "• City: City in which restaurant is located\n",
    "• Address: Address of the restaurant\n",
    "• Locality: Location in the city\n",
    "• Locality Verbose: Detailed description of the locality\n",
    "• Longitude: Longitude coordinate of the restaurant&#39;s location\n",
    "• Latitude: Latitude coordinate of the restaurant&#39;s location\n",
    "• Cuisines: Cuisines offered by the restaurant\n",
    "• Average Cost for two: Cost for two people in different currencies ��\n",
    "• Currency: Currency of the country\n",
    "• Has Table booking: yes/no\n",
    "• Has Online delivery: yes/ no\n",
    "• Is delivering: yes/ no\n",
    "• Switch to order menu: yes/no\n",
    "• Price range: range of price of food\n",
    "• Aggregate Rating: Average rating out of 5\n",
    "• Rating color: depending upon the average rating color\n",
    "• Rating text: text on the basis of rating of rating\n",
    "• Votes: Number of ratings casted by people\n",
    "\n",
    "Problem statement : In this dataset predict 2 things –\n",
    "1) Average Cost for two\n",
    "2) Price range\n",
    "\n",
    "Hint : Use pandas merge operation -- pd.merge (df1,df2) to combine two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fa0d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install opendatasets --upgrade\n",
    "import opendatasets as od"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1ba395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\"C://Users//win 7//Desktop//Datascience//Zomato.csv\",engine='python')\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624d5e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if dataset contains any null\n",
    "\n",
    "nan_values = df.isna()\n",
    "nan_columns = nan_values.any()\n",
    "\n",
    "columns_with_nan = df.columns[nan_columns].tolist()\n",
    "print(columns_with_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53045a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_excel('C://Users//win 7//Desktop//Datascience//Country-Code.xlsx')\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258e2e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.merge(df,df1,on='Country Code',how='left')\n",
    "df2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de25d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('List of counteris the survey is spread accross - ')\n",
    "for x in pd.unique(df2.Country): print(x)\n",
    "print()\n",
    "print('Total number to country', len(pd.unique(df2.Country)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b72306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.offline import init_notebook_mode, plot, iplot\n",
    "\n",
    "labels = list(df2.Country.value_counts().index)\n",
    "values = list(df2.Country.value_counts().values)\n",
    "\n",
    "fig = {\n",
    "    \"data\":[\n",
    "        {\n",
    "            \"labels\" : labels,\n",
    "            \"values\" : values,\n",
    "            \"hoverinfo\" : 'label+percent',\n",
    "            \"domain\": {\"x\": [0, .9]},\n",
    "            \"hole\" : 0.6,\n",
    "            \"type\" : \"pie\",\n",
    "            \"rotation\":120,\n",
    "        },\n",
    "    ],\n",
    "    \"layout\": {\n",
    "        \"title\" : \"Zomato's Presence around the World\",\n",
    "        \"annotations\": [\n",
    "            {\n",
    "                \"font\": {\"size\":20},\n",
    "                \"showarrow\": True,\n",
    "                \"text\": \"Countries\",\n",
    "                \"x\":0.2,\n",
    "                \"y\":0.9,\n",
    "           },\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "iplot(fig)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdc3f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.groupby(['Aggregate rating','Rating color', 'Rating text']).size().reset_index().rename(columns={0:'Rating Count'})\n",
    "df3\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18694f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "matplotlib.rcParams['font.size'] = 14\n",
    "matplotlib.rcParams['figure.figsize'] = (9, 5)\n",
    "matplotlib.rcParams['figure.facecolor'] = '#00000000'\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "# plt.xticks(rotation=75)\n",
    "plt.title('Rating Color')\n",
    "sns.barplot(x=df3['Rating color'], y=df3['Rating Count']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3b60a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "No_rating = df2[df2['Rating color']=='White'].groupby('Country').size().reset_index().rename(columns={0:'Rating Count'})\n",
    "No_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7686aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_currency = df2[['Country','Currency']].groupby(['Country','Currency']).size().reset_index(name='count').drop('count', axis=1, inplace=False)\n",
    "country_currency.sort_values('Currency').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eddfcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.title('Online Delivery Distribution')\n",
    "plt.pie(df2['Has Online delivery'].value_counts()/9551*100, labels=df2['Has Online delivery'].value_counts().index, autopct='%1.1f%%', startangle=180);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a23496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.offline import init_notebook_mode, plot, iplot\n",
    "import plotly.graph_objs as go\n",
    "plt.figure(figsize=(12,6))\n",
    "# import plotly.plotly as py\n",
    "\n",
    "labels = list(df2.City.value_counts().head(20).index)\n",
    "values = list(df2.City.value_counts().head(20).values)\n",
    "\n",
    "fig = {\n",
    "    \"data\":[\n",
    "        {\n",
    "            \"labels\" : labels,\n",
    "            \"values\" : values,\n",
    "            \"hoverinfo\" : 'label+percent',\n",
    "            \"domain\": {\"x\": [0, .9]},\n",
    "            \"hole\" : 0.6,\n",
    "            \"type\" : \"pie\",\n",
    "            \"rotation\":120,\n",
    "        },\n",
    "    ],\n",
    "    \"layout\": {\n",
    "        \"title\" : \"Zomato's Presence Citywise\",\n",
    "        \"annotations\": [\n",
    "            {\n",
    "                \"font\": {\"size\":20},\n",
    "                \"showarrow\": True,\n",
    "                \"text\": \"Cities\",\n",
    "                \"x\":0.2,\n",
    "                \"y\":0.9,\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "iplot(fig);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be262f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "Delhi = df2[(df2.City == 'New Delhi')]\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(x=Delhi.Locality.value_counts().head(10), y=Delhi.Locality.value_counts().head(10).index)\n",
    "\n",
    "plt.ylabel(None);\n",
    "plt.xlabel('Number of Resturants')\n",
    "plt.title('Resturants Listing on Zomato');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "312cca9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Delhi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\WIN7~1\\AppData\\Local\\Temp/ipykernel_588/489753476.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m## Fetching the resturants having 'Excellent' and 'Very Good' rating\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mConnaughtPlace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDelhi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDelhi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLocality\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Connaught Place'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mDelhi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Rating text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Excellent'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Very Good'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mConnaughtPlace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConnaughtPlace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCuisines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Delhi' is not defined"
     ]
    }
   ],
   "source": [
    "# I achieve this by the following steps\n",
    "\n",
    "## Fetching the resturants having 'Excellent' and 'Very Good' rating\n",
    "ConnaughtPlace = Delhi[(Delhi.Locality.isin(['Connaught Place'])) & (Delhi['Rating text'].isin(['Excellent','Very Good']))]\n",
    "\n",
    "ConnaughtPlace = ConnaughtPlace.Cuisines.value_counts().reset_index()\n",
    "\n",
    "## Extracing all the cuisens in a single list\n",
    "cuisien = []\n",
    "for x in ConnaughtPlace['index']: \n",
    "  cuisien.append(x)\n",
    "\n",
    "# cuisien = '[%s]'%', '.join(map(str, cuisien))\n",
    "cuisien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd40c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "  \n",
    "comment_words = ''\n",
    "stopwords = set(STOPWORDS)\n",
    "  \n",
    "# iterate through the csv file\n",
    "for val in cuisien:\n",
    "      \n",
    "    # typecaste each val to string\n",
    "    val = str(val)\n",
    "  \n",
    "    # split the value\n",
    "    tokens = val.split()\n",
    "      \n",
    "    # Converts each token into lowercase\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = tokens[i].lower()\n",
    "      \n",
    "    comment_words += \" \".join(tokens)+\" \"\n",
    "  \n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                stopwords = stopwords,\n",
    "                min_font_size = 10).generate(comment_words)\n",
    "\n",
    "# plot the WordCloud image                       \n",
    "plt.figure(figsize = (8, 8), facecolor = 'b', edgecolor='g')\n",
    "plt.title('Resturants cuisien -  Top Resturants')\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25c12cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_locality = Delhi.Locality.value_counts().head(10)\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "plt.figure(figsize=(12,6))\n",
    "ax = sns.countplot(y= \"Locality\", hue=\"Has Online delivery\", data=Delhi[Delhi.Locality.isin(top_locality.index)])\n",
    "plt.title('Resturants Online Delivery');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548fb7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.scatterplot(x=\"Average Cost for two\", y=\"Aggregate rating\", hue='Price range', data=Delhi)\n",
    "\n",
    "plt.xlabel(\"Average Cost for two\")\n",
    "plt.ylabel(\"Aggregate rating\")\n",
    "plt.title('Rating vs Cost of Two');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f3d7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Delhi['Rating text'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4770bfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "Highly_rated = Delhi[Delhi['Rating text'].isin(['Excellent'])]\n",
    "\n",
    "fig = px.scatter_mapbox(Highly_rated, lat=\"Latitude\", lon=\"Longitude\", hover_name=\"City\", hover_data=[\"Aggregate rating\", \"Restaurant Name\"],\n",
    "                        color_discrete_sequence=[\"fuchsia\"], zoom=10, height=300)\n",
    "fig.update_layout(mapbox_style=\"open-street-map\")\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.update_layout(title='Highle rated Resturants Location',\n",
    "                  autosize=True,\n",
    "                  hovermode='closest',\n",
    "                  showlegend=False)\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=800,\n",
    "    height=500,)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f76366",
   "metadata": {},
   "outputs": [],
   "source": [
    "types = {\n",
    "    \"Breakfast and Coffee\" : [\"Cafe Coffee Day\", \"Starbucks\", \"Barista\", \"Costa Coffee\", \"Chaayos\", \"Dunkin' Donuts\"],\n",
    "    \"American\": [\"Domino's Pizza\", \"McDonald's\", \"Burger King\", \"Subway\", \"Dunkin' Donuts\", \"Pizza Hut\"],\n",
    "    \"Ice Creams and Shakes\": [\"Keventers\", \"Giani\", \"Giani's\", \"Starbucks\", \"Baskin Robbins\", \"Nirula's Ice Cream\"]\n",
    "}\n",
    "\n",
    "breakfast = Delhi[Delhi['Restaurant Name'].isin(types['Breakfast and Coffee'])]\n",
    "american = Delhi[Delhi['Restaurant Name'].isin(types['American'])]\n",
    "ice_cream = Delhi[Delhi['Restaurant Name'].isin(types['Ice Creams and Shakes'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47317a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "breakfast = breakfast[['Restaurant Name','Aggregate rating']].groupby('Restaurant Name').mean().reset_index().sort_values('Aggregate rating',ascending=False)\n",
    "breakfast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24681342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "df= breakfast\n",
    "fig = px.bar(df, y='Aggregate rating', x='Restaurant Name', text='Aggregate rating', title=\"Breakfast and Coffee locations\")\n",
    "fig.update_traces(texttemplate='%{text:.3s}', textposition='outside')\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=800,\n",
    "    height=500,)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb0f7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "american = american[['Restaurant Name','Aggregate rating']].groupby('Restaurant Name').mean().reset_index().sort_values('Aggregate rating',ascending=False)\n",
    "american"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e476660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "df= american\n",
    "fig = px.bar(df, y='Aggregate rating', x='Restaurant Name', text='Aggregate rating', title=\"Fast Food Resturants\")\n",
    "fig.update_traces(texttemplate='%{text:.3s}', textposition='outside')\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=800,\n",
    "    height=500,)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c82925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ice_cream = ice_cream[['Restaurant Name','Aggregate rating']].groupby('Restaurant Name').mean().reset_index().sort_values('Aggregate rating',ascending=False)\n",
    "ice_cream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db393593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "df= ice_cream\n",
    "fig = px.bar(df, y='Aggregate rating', x='Restaurant Name', text='Aggregate rating', title=\"Ice Cream Parlours\")\n",
    "fig.update_traces(texttemplate='%{text:.3s}', textposition='outside')\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=800,\n",
    "    height=500,)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb852ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
